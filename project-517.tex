\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
%\usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{SelfExplain Text Classifier Reproducibility Report}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Murray Kang,  Xuweiyi Chen,  Hao Luo \\ % write a list of authors
  University of Washington \\
 \texttt{\{haoqik, xuweic, author3\}@uw.edu} \\ % write your email
}

\begin{document}

\maketitle

% Template and style guide for the eproducibility project for CSE 517.

% Note that we slightly updated the style file for the CSE 517 project, so it is not exactly the same as 2020 ML Reproducibility Challenge (or later iterations).  In order to submit to a ML Reproducibility Challenge, please move the report to the official template, and get advice from the TA to make sure that your format is good.

\section*{\centering Reproducibility Summary}


\subsection*{Scope of Reproducibility}

We chose to reproduce the model SelfExplain which is a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. There are two speical layers added in this model, a globally interpretable layer and a locally interpretable layer. In the globally interpretable layer, it aims to figure out the most influential concepts on the model's prediction in the training set for each data sample. In the locally interpretable layer, it aims to calculate the relevance score of each input concept that is relative to the prediciton to quantify the contributions.

\subsection*{Methodology}

First of all, we used the author's code to train the model. However, the code provided by 
the author is far from complete. We encountered and investigate numerous bugs when we follow
the instructions provided by the author. Even we successful trained the model with the provided 
code after we fixed all the bugs, the evaluation and test part of the code is also incomplete. 
We spent some extra time in order to follow the instructions from the paper.

\subsection*{Results}

[TODO] Start with your overall conclusion---where was your study successful and where not successful? Be specific and use precise language, e.g.,``we reproduced the accuracy to within 1\% of reported value, that upholds the paper's conclusion that it performs much better than baselines.'' Getting exactly the same number is in most cases infeasible, so you'll need to use your judgment to decide if your results support the original claim of the paper. 

\subsection*{What was Easy}

[TODO] Describe which parts of your reproduction study were easy. E.g., was it easy to run the author's code, or easy to reimplement their method based on the description in the paper. The goal of this section is to summarize to the reader which parts of the original paper they could easily apply to their problem. 

\subsection*{What was Difficult}

[TODO] * Honestly, we faced many difficulties in the process of reproducing the experiment. Maybe due to the difference and incompatibility of our PCs with the authors' code, we used 3 more days to revise and debug the original code to make it able to run on our end. In the authors' code, the test set of the original dataset they used is not preprocessed. To testify our result, we also preprocessed the test set of the original dataset and revised the original codes to figure out the results on test set.

Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to reuse, or require a significant amount of work and resources to verify. 

\subsection*{Communication with Original Authors}

[TODO] Briefly describe how much (if any contact) you had with the original authors.

\newpage

% Keep in mind that your page limit is 8, excluding references and the summary section above.
% For specific grading rubrics, please see the project instructions.

\section{Introduction}

SelfExplain is a neural text classifier that can explain the text prediction using phrase-base concepts.
The significance of this paper about SelfExplain is improving the interpretibilities of Neural 
network. In many areas including NLP, people always want to understand the model decision making
procedure of Neural network models. In the area of Trustworthy AI, people found many bias 
that has been encapsulated in Neural network models. This paper combines two common approaches 
for better interpretibilities: (1) explain predictions based on the internal of model; (2) built-in 
interpretable layer. From our pespectives, this paper proposed two layers in order to provide 
better transparency for transformer-based NLP classifier: (1) Locally interpretable Layer 
analysis the contribution of each phrase to the final label distribution; (2) Globally interpretable 
Layer search the most influential concept from the training data for each input. Note that 
these two layers can be combined with transformer-based classifier and provide even better 
performance compared with only use transformer-based classifier. It suggests to people that 
these two layers facilitates better interpretibility and improve the trustworthyness.

It appears to us that this methodology might be able to convert for other Neural network if we 
consider each feature captured by YOLO as scores. However, it requires a lot of engineering and 
deeper knowledge. This paper suggests a innovative way of promote interpretibility of Neural 
model by introducing search-based and similarity-based layers. It is also interesting to discuss 
why the prediciton power increases by incorporating LIL and GIL layer.

\section{Scope of Reproducibility}

Since this paper is largely regarding on interpretibility, we want to see its interpretibility. However,
we only have three people. The best practice is to test stability of the top interpreted words.
In other words, we will train a model and utilize the model to predict a set of very similary sentences.

Then we want to test XLNet-Base model and RoBERTa-Base model with LIL and GIL layers. We want to see 
the performances of the models do not suffer as the paper claimed.

Since the original dataset provides only SST-2, we want to train this model across SST-5 and 
TREC-6 and compare the accuracy I acquired and the data from the paper. If time permits, we 
will train this model with extra datasets. 

\subsection{Addressed Claims from the Original Paper} \label{claims}

\begin{enumerate}
    \item The transformer-based classifier with LIL and GIL layers can interpret sentences stably.
    \item The transformer-based classifier with LIL and GIL layers perform better than only transformer
    based classifier in terms of accuracy
    \item The transformer-based classifier with LIL and GIL layers perform consistently better 
    across datasets with fixed hyperparameters.
\end{enumerate}


\section{Methodology}

We used the author's code, but the author's code is incomplete. We build upon the code 
provided by the author. We provide clear documentations for other people to use and we 
benchmark and develop our experiments on Google Cloud Platform. We will tailor our model
to attu if time permits.


\subsection{Model Descriptions}

SelfExplain is a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. In the SelfExplain architecture, firstly, there is a transformer encoder that encoded the input and its relative non-terminals. After that, it used three layers after regularization to make prediction, a linear layer, a globally interpretable layer and a locally interpretable layer. \\
In the globally interpretable layer, it aims to figure out the most influential concepts on the model's prediction in the training set for each data sample. To be more specific, firstly, there is a concept store that contains all concepts in the training set, in which each candidate $q_k$ is represented by a mean pooled representation of its constituent words. $$q_k = \frac{\sum_{w \in q_k}{e(w)}}{len(q_k)}  $$ Since the model is a finetuned for downstream task, all candidate representation $q_k$ are re-indexed after every fixed number of steps. 
Next, the model used the Maximum Inner Product Search (MIPS) to retrieve the top $K$ influenced concepts ${q_k}_{1:K}$ from the concept store based on the cosine similarity function: 
$$d(x, Q) = \frac{\mathbf{x} \cdot q}{ \|\mathbf{x}\| \|q\|}  \qquad  \forall q \in Q$$
After identifying the influential concepts from the training data, the model learned it end-to-end via back propagation. Our inner product model for this globally interpretable layer is defined as follows:
$$  p(q|\mathbf{x}_i) = \frac{exp(d(\mathbf{u}_\mathbb{S}, q))}{\sum_{q'} exp(d(\mathbf{u}_\mathbb{S}, q'))}$$

In the locally interpretable layer, it aims to calculate the relevance score of each input concept that is relative to the prediction to quantify the contributions. Firstly, it computes a local relevance score for all input concepts from a sample. For each non-terminal $nt_j$, there is a score that qualifies the contribution of each $nt_j$ to the label and then the model compared it to the contribution of the root node $nt_\mathbb{s}$. Finally, it chose the phrases $\mathbf{C}_L$ that contributed most to explain the predictions locally. To be more specific, given a encoder, LIL computes the contribution from $nt_j$ to the final prediction, and then it built a representation of the input without contribution of phrase $nt_j$ and use it to score labels:
$$t_j = g(\mathbf{u}_j) - g(\mathbf{u}_\mathbb{S})$$
$$s_j = softmax(\mathbf{W_v} \times t_j + \mathbf{b}_v)$$
where g is a $relu$ activation function, $t_j \in \mathbb{R}^D$, $s_j \in \mathbb{R}^C$, $\mathbf{W}_v \in \mathbb{R}^{D \times C}$, $s_j$ is a label distribution without the contribution of $nt_j$. \\
Given this, the relevance score $\mathbf{r}_j$ is the given by the difference between the classifier score fore the predicted label based on entire input and the label score based on the input without $nt_j$:
$$\mathbf{r}_j = (l_Y)_{i|i=P_c} - (s_j)_{i|i=P_c}$$

\subsection{Datasets}

We used SST-2, SST-5 and TREC-6 fine-grained classification dataset from the Stanform NLP website and DeepAI website for 
benchmark and perform experiments for each of three claims we discussed in the section of 
Scope of Reproducibility.

\subsection{Hyperparameters}
Describe how you set the hyperparameters and what was the source for their value (e.g., paper, code, or your guess). 

\subsection{Implementation}

We made some changes based on the the author's code. Also, we used different dataset (SST- 5) to re-implement the experiment mentioned in the paper. The original author's code link: https://github.com/dheerajrajagopal/SelfExplain

\subsection{Experimental Setup}

[TODO] Explain how you ran your experiments, e.g. the CPU/GPU resources and provide the link to your code and notebooks. 

\subsection{Computational Requirements}

The original paper had access to two NVIDIA V-100 GPUs, but we only have access to either 
RTX 3080 and one K-80 provided by GCP. However, the model is too large for RTX 3080 and therefore we will 
primary train the model using K-80.

\section{Results}

[TODO] Start with a high-level overview of your results. Does your work support the claims you listed in section \ref{claims}? Keep this section as factual and precise as possible, reserve your judgment and discussion points for the ``Discussion'' section that comes later. 

Go into each individual result you have, say how it relates to one of the claims and explain what your result is. Logically group related results into sections. Clearly state if you have gone beyond the original paper to run additional experiments and how they relate to the original claims. 

Tip 1: Be specific and use precise language, e.g. ``we reproduced the accuracy to within 1\% of reported value; that upholds the paper's conclusion that it performs much better than baselines.'' Getting exactly the same number is in most cases infeasible, so you'll need to use your judgment to decide if your results support the original claim of the paper. 

Tip 2: You may want to use tables and figures to demonstrate your results.

% The number of subsections for results should be the same as the number of hypotheses you are trying to verify.

\subsection{Result 1}
[TODO]
\subsection{Result 2}
[TODO]
\subsection{Additional Results not Present in the Original Paper}

[TODO] Describe any additional experiments beyond the original paper. This could include experimenting with additional datasets, exploring different methods, running more ablations, or tuning the hyperparameters. For each additional experiment, clearly describe which experiment you conducted, its result, and discussion (e.g., what is the indication of the result).

\section{Discussion}

[TODO] Describe larger implications of the experimental results, whether the original paper was reproducible, and if it wasnâ€™t, what factors you believe made it irreproducible. 

Give your judgment on whether  the evidence you got from your experiments supports the claims of the paper. Discuss the strengths and weaknesses of your approach---perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was Easy}

[TODO] Describe which parts of your reproduction study were easy. E.g., was it easy to run the author's code, or easy to reimplement their method based on the description in the paper. The goal of this section is to summarize to the reader which parts of the original paper they could easily apply to their problem. 

Tip: Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g., code had extensive API documentation and a lot of examples that matched experiments in papers). 

\subsection{What was Difficult}

[TODO] Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to reuse, or require a significant amount of work and resources to verify. 

Tip: Be careful to put your discussion in context. For example, don't say ``the math was difficult to follow,'' say ``the math requires advanced knowledge of calculus to follow.'' 

\subsection{Recommendations for Reproducibility}

[TODO] Describe a set of recommendations to the original authors or others who work in this area for improving reproducibility.

\section*{Communication with Original Authors}

[TODO] Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback.


\section*{References}

[TODO] Use bibtex and check its output; manual corrections are often necessary.

\end{document}
