\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
%\usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{SelfExplain Text Classifier Reproducibility Report}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Murray Kang,  Xuweiyi Chen,  Hao Luo \\ % write a list of authors
  University of Washington \\
 \texttt{\{haoqik, xuweic, author3\}@uw.edu} \\ % write your email
}

\begin{document}

\maketitle

% Template and style guide for the eproducibility project for CSE 517.

% Note that we slightly updated the style file for the CSE 517 project, so it is not exactly the same as 2020 ML Reproducibility Challenge (or later iterations).  In order to submit to a ML Reproducibility Challenge, please move the report to the official template, and get advice from the TA to make sure that your format is good.

\section*{\centering Reproducibility Summary}


\subsection*{Scope of Reproducibility}

We chose to reproduce the model SelfExplain which is a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. There are two speical layers added in this model, a globally interpretable layer and a locally interpretable layer. In the globally interpretable layer, it aims to figure out the most influential concepts on the model's prediction in the training set for each data sample. In the locally interpretable layer, it aims to calculate the relevance score of each input concept that is relative to the prediciton to quantify the contributions.

\subsection*{Methodology}

First of all, we used the author's code to train the model. However, the code provided by 
the author is far from complete. We encountered and investigate numerous bugs when we follow
the instructions provided by the author. Even we successful trained the model with the provided 
code after we fixed all the bugs, the evaluation and test part of the code is also incomplete. 
We spent some extra time in order to follow the instructions from the paper.

\subsection*{Results}

[TODO] Start with your overall conclusion---where was your study successful and where not successful? Be specific and use precise language, e.g.,``we reproduced the accuracy to within 1\% of reported value, that upholds the paper's conclusion that it performs much better than baselines.'' Getting exactly the same number is in most cases infeasible, so you'll need to use your judgment to decide if your results support the original claim of the paper. 

\subsection*{What was Easy}

Since the code we used was based on the authors', it helped us better understand the structure and ideas of SelfExplain, as well as all the concepts mentioned in the paper. For the transformers we used in the implementation, there are some examples and documentations of the API we need to use, so it really helped a lot in solving several problems about the codes. In terms of the ideas of the paper, using interpretable layer to interpret a sentence is intuitive. For LIL layer, it is intuitive that we can use some key phrases (which are concepts in this paper) to summarize or represent one sentence, which is what the LIL layer does. For GIL layer, it is also not hard to understand that the goal of it is that we want to retrieve some of most relevant phrases that are seen or learned (in the training set) to represent or interpret a sentence.  

\subsection*{What was Difficult}

Honestly, we faced many difficulties in the process of reproducing the experiment. Maybe due to the difference and incompatibility of our PCs with the authors' code, it took us many days to revise and debug the original code to make it able to run on our end. Also, there are few documentations and information that could help us change the transformer to RoBERTa on the Internet, which is needed for our additional experiment. In terms of the ideas of the paper, although the construction of the two interpretable layers is intuitive, the reason why and how the interpretable helps improve the performance of the classifier is still hard to understand.

\subsection*{Communication with Original Authors}

[TODO] Briefly describe how much (if any contact) you had with the original authors.

\newpage

% Keep in mind that your page limit is 8, excluding references and the summary section above.
% For specific grading rubrics, please see the project instructions.

\section{Introduction}

SelfExplain is a neural text classifier that can explain the text prediction using phrase-base concepts.
The significance of this paper about SelfExplain is improving the interpretibilities of Neural 
network. In many areas including NLP, people always want to understand the model decision making
procedure of Neural network models. In the area of Trustworthy AI, people found many bias 
that has been encapsulated in Neural network models. This paper combines two common approaches 
for better interpretibilities: (1) explain predictions based on the internal of model; (2) built-in 
interpretable layer. From our pespectives, this paper proposed two layers in order to provide 
better transparency for transformer-based NLP classifier: (1) Locally interpretable Layer 
analysis the contribution of each phrase to the final label distribution; (2) Globally interpretable 
Layer search the most influential concept from the training data for each input. Note that 
these two layers can be combined with transformer-based classifier and provide even better 
performance compared with only use transformer-based classifier. It suggests to people that 
these two layers facilitates better interpretibility and improve the trustworthyness.

It appears to us that this methodology might be able to convert for other Neural network if we 
consider each feature captured by YOLO as scores. However, it requires a lot of engineering and 
deeper knowledge. This paper suggests a innovative way of promote interpretibility of Neural 
model by introducing search-based and similarity-based layers. It is also interesting to discuss 
why the prediciton power increases by incorporating LIL and GIL layer.

\section{Scope of Reproducibility}

Since this paper is largely regarding on interpretibility, we want to see its interpretibility. However,
we only have three people. The best practice is to test stability of the top interpreted words.
In other words, we will train a model and utilize the model to predict a set of very similary sentences.

Then we want to test XLNet-Base model and RoBERTa-Base model with LIL and GIL layers. We want to see 
the performances of the models do not suffer as the paper claimed.

Since the original dataset provides only SST-2, we want to train this model across SST-5 and 
TREC-6 and compare the accuracy I acquired and the data from the paper. If time permits, we 
will train this model with extra datasets. 

\subsection{Addressed Claims from the Original Paper} \label{claims}

\begin{enumerate}
    \item The transformer-based classifier with LIL and GIL layers can interpret sentences stably.
    \item The transformer-based classifier with LIL and GIL layers perform better than only transformer
    based classifier in terms of accuracy
    \item The transformer-based classifier with LIL and GIL layers perform consistently better 
    across datasets with fixed hyperparameters.
\end{enumerate}


\section{Methodology}

We used the author's code, but the author's code is incomplete. We build upon the code 
provided by the author. We provide clear documentations for other people to use and we 
benchmark and develop our experiments on Google Cloud Platform. We will tailor our model
to attu if time permits.


\subsection{Model Descriptions}

SelfExplain is a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. In the SelfExplain architecture, firstly, there is a transformer encoder that encoded the input and its relative non-terminals. After that, it used three layers after regularization to make prediction, a linear layer, a globally interpretable layer and a locally interpretable layer. \\
In the globally interpretable layer, it aims to figure out the most influential concepts on the model's prediction in the training set for each data sample. To be more specific, firstly, there is a concept store that contains all concepts in the training set, in which each candidate $q_k$ is represented by a mean pooled representation of its constituent words. $$q_k = \frac{\sum_{w \in q_k}{e(w)}}{len(q_k)}  $$ Since the model is a finetuned for downstream task, all candidate representation $q_k$ are re-indexed after every fixed number of steps. 
Next, the model used the Maximum Inner Product Search (MIPS) to retrieve the top $K$ influenced concepts ${q_k}_{1:K}$ from the concept store based on the cosine similarity function: 
$$d(x, Q) = \frac{\mathbf{x} \cdot q}{ \|\mathbf{x}\| \|q\|}  \qquad  \forall q \in Q$$
After identifying the influential concepts from the training data, the model learned it end-to-end via back propagation. Our inner product model for this globally interpretable layer is defined as follows:
$$  p(q|\mathbf{x}_i) = \frac{exp(d(\mathbf{u}_\mathbb{S}, q))}{\sum_{q'} exp(d(\mathbf{u}_\mathbb{S}, q'))}$$

In the locally interpretable layer, it aims to calculate the relevance score of each input concept that is relative to the prediction to quantify the contributions. Firstly, it computes a local relevance score for all input concepts from a sample. For each non-terminal $nt_j$, there is a score that qualifies the contribution of each $nt_j$ to the label and then the model compared it to the contribution of the root node $nt_\mathbb{S}$. Finally, it chose the phrases $\mathcal{C}_L$ that contributed most to explain the predictions locally. To be more specific, given a encoder, LIL computes the contribution from $nt_j$ to the final prediction, and then it built a representation of the input without contribution of phrase $nt_j$ and use it to score labels:
$$t_j = g(\mathbf{u}_j) - g(\mathbf{u}_\mathbb{S})$$
$$s_j = \textrm{softmax}(\mathbf{W}_v \times t_j + \mathbf{b}_v)$$
where g is a $relu$ activation function, $t_j \in \mathbb{R}^D$, $s_j \in \mathbb{R}^C$, $\mathbf{W}_v \in \mathbb{R}^{D \times C}$, $s_j$ is a label distribution without the contribution of $nt_j$. \\
Given this, the relevance score $\mathbf{r}_j$ is the given by the difference between the classifier score fore the predicted label based on entire input and the label score based on the input without $nt_j$:
$$\mathbf{r}_j = (l_Y)_{i|i=P_c} - (s_j)_{i|i=P_c}$$

[Training process]

SelfExplain is trained to maximize the conditional log-likelihood of predicting the class at final layers: LIL, GIL and linear (for label prediction). It has been found that regularizing models with explanation specific losses improves inherently interpretable models for local interpretability (Melis and Jaakkola,
2018). SelfExplain extends this idea for both global and local interpretable output for our classifier model, so we regularize the loss through GIL and LIL layers by optimizing their output for the end-task as well. 

For the LIL layer, we use a weighted aggregated representation over $s_j$ and compute the log-likelihood loss as follows:
$$l_L = \sum_{j, j \neq \mathbb{S}}{w_{sj} \times s_j}, w_{sj} \in \mathbb{R}$$
and the overall loss of the whole training set is as follows:
$$\mathcal{L}_L = - \sum_{c=1}^{C}{log(l_L)} \times {y_c}$$

For the GIL layer, we firstly aggregate the scores over all the retrieved $q_{1:K}$ as a weighted sum, then pass it through an activation layer, a linear layer, and a softmax layer to compute the log-likelihood loss as follows:
$$l_G = \textrm{softmax}(\mathbf{W}_u \times g(\sum_{k=1}^{K}{\mathbf{w}_k \times q_k}) + \textbf{b}_u)$$
and where $\mathbf{W}_u \in \mathbb{R}^{D \times C}$, $\mathbf{w}_k \in \mathbb{R}$ and $g$ represents $relu$ activation function. Here, the global interpretable concepts are denoted by $\mathcal{C}_G = q_{1:K}$, and $l_G$ represents the softmax for the GIL layer. The overall loss of the whole training set is as follows:
$$\mathcal{L}_G = - \sum_{c=1}^{C}{log(l_G)} \times {y_c}$$

In the training process, we optimize for the following joint loss:
$$\mathcal{L} = \alpha \times \mathcal{L}_G + \beta \times \mathcal{L}_L + \mathcal{L}_Y$$ 
where $\mathcal{L}_Y = - \sum_{c=1}^{C}{log(l_Y)} \times {y_c}$ is the overall loss of the whole training set for the linear layer . Here, $\alpha$ is and $\beta$ are regularization hyper-parameters. All loss components use cross-entropy loss based on task label $y_c$.



\subsection{Datasets}

We used SST-2, SST-5 and TREC-6 fine-grained classification dataset from the Stanform NLP website and DeepAI website for 
benchmark and perform experiments for each of three claims we discussed in the section of 
Scope of Reproducibility.

\subsection{Hyperparameters}
Describe how you set the hyperparameters and what was the source for their value (e.g., paper, code, or your guess). 

\subsection{Implementation}

We made some changes based on the the author's code. Also, we used different dataset (SST- 5) to re-implement the experiment mentioned in the paper. The original author's code link: https://github.com/dheerajrajagopal/SelfExplain

\subsection{Experimental Setup}

***Github

Following the paper, we used two different transformer encoders configurations as our base model: RoBERTa (Liu et al., 2019) - a robustly optimized version of BERT and XLNet (Yang et al., 2019) - a variant model based on Transformer-XL architecture. Then, we used the two encoders without the GIL and LIL layers as the baselines, and we generated parse tree to extract target concepts for the input. Also, we maintained the weights and hyperparameters from the pre-trained encoders, and fine-tuned the parameters on the GIL and LIL modules. 

We experimented on two different numbers of global
influential concepts K $= \{5, 10\}$, different transformer encoders, different combinations of layers used. We performed all experiments on two separated K-80 GPUs on Google Cloud Platform. 


[TODO] Explain how you ran your experiments, e.g. the CPU/GPU resources and provide the link to your code and notebooks. 

\subsection{Computational Requirements}

The original paper had access to two NVIDIA V-100 GPUs, but we only have access to either 
RTX 3080 and one K-80 provided by GCP. However, the model is too large for RTX 3080 and therefore we will 
primary train the model using K-80.

\section{Results}

[TODO] Start with a high-level overview of your results. Does your work support the claims you listed in section \ref{claims}? Keep this section as factual and precise as possible, reserve your judgment and discussion points for the ``Discussion'' section that comes later. 

Go into each individual result you have, say how it relates to one of the claims and explain what your result is. Logically group related results into sections. Clearly state if you have gone beyond the original paper to run additional experiments and how they relate to the original claims. 

Tip 1: Be specific and use precise language, e.g. ``we reproduced the accuracy to within 1\% of reported value; that upholds the paper's conclusion that it performs much better than baselines.'' Getting exactly the same number is in most cases infeasible, so you'll need to use your judgment to decide if your results support the original claim of the paper. 

Tip 2: You may want to use tables and figures to demonstrate your results.

% The number of subsections for results should be the same as the number of hypotheses you are trying to verify.

\subsection{Result 1}
[TODO]
\subsection{Result 2}
[TODO]
\subsection{Additional Results not Present in the Original Paper}

[TODO] Describe any additional experiments beyond the original paper. This could include experimenting with additional datasets, exploring different methods, running more ablations, or tuning the hyperparameters. For each additional experiment, clearly describe which experiment you conducted, its result, and discussion (e.g., what is the indication of the result).

\section{Discussion}

[TODO] Describe larger implications of the experimental results, whether the original paper was reproducible, and if it wasnâ€™t, what factors you believe made it irreproducible. 

Give your judgment on whether  the evidence you got from your experiments supports the claims of the paper. Discuss the strengths and weaknesses of your approach---perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was Easy}

Thanks to authors' codes, which built up the foundation of the SelfExplain model, when we faced any confusion about the concepts mentioned in the paper, we can refer to the codes to deepen our understanding of the model. Also, the code style of the authors is clear and easy for us to understand the ideas of each part. Next, the dataset used in the experiments are easy to obtain since they are all open sources on the websites. In the training step, the authors used PyTorch lightning library to build the Trainer, and there were a lot of clear examples and API documentation which helped us better understand the structure of the codes.  

\subsection{What was Difficult}

We experienced several challenges when reproducing this paper. In the process of reading through the whole paper, there are some concepts and intuitive that are hard for us to understand. Using maximum inner product search (MIPS) to retrieve the top relevant K concepts from the concept store is one of them. Therefore, we had to read the paper of MIPS (Guu etal.), and found out the methodology and intuitive of it. The next one concerning the methodology of the paper is the reason of using regularization with the log-likelihood loss from LIL and GIL in training the linear classification layer. So, we read through the paper regarding the regularizing models with explanation losses to figure it out.(Melis and Jaakkola, 2018). 

In terms of the difficulty of coding, there were still some challenges we faced. Firstly, the authors' codes were broken, and we had to fix it in the a couple of weeks. Also, we established two computing K-80 GPUs on Google Cloud Platform to train the models. Also, the codes for RoBERTa transformer is not completed, and there are few useful information we can find on the websites that are suitable to our case. Also, the authors cannot access to their original codes for RoBERTa. Thus, it took us many days to figure out how to use RoBERTa in the SelfExplain model by multiple searching and experiments.\\ 

\subsection{Recommendations for Reproducibility}

In terms of the concepts, ideas, and intuitive in the paper, We recommend that it would be better to read the paper (Melis and Jaakkola 2018 that discussed and proofed why regularizing models with explanation certain losses can improve interpretable models for local interpretability in the area of computer vision. The SelfExplain model smartly extended this idea for both global and local interpretable output for the classifier model, and showed the actual improvement of doing this. 

Since there were many incompatibilities happened with the authors' code and our local Python interpreter, for the researchers who would use the author's code, in which some methods and packages are out-of-date, we recommend you need to install the same version of the packages used if possible. For those packages that the specific old version cannot be installed anyone, I recommend you can refer to our codes to revise them on your end. 

Also, for the researchers who do not have access to any powerful computing GPU, we recommend you can use Google Cloud Platform to train the model, espcifally for the dataset of SST-2, which has 60,000 sentences, since it would took a relative long time. 
[TODO] Describe a set of recommendations to the original authors or others who work in this area for improving reproducibility.

\section*{Communication with Original Authors}

[TODO] Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback.


\section*{References}

[TODO] Use bibtex and check its output; manual corrections are often necessary.
\end{document}
